{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-25T22:38:48.240494Z",
     "start_time": "2023-10-25T22:38:45.910399Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "# from pandas.core.common import SettingWithCopyWarning\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.io import arff\n",
    "import models\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "path = '/Users/davidtigau/Documents/Courses/Research Topics in Data Mining/Assignments/Research Paper/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-25T22:38:48.254499Z",
     "start_time": "2023-10-25T22:38:48.244832Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(df):\n",
    "    #num of columns with majority NaN values\n",
    "    counter = 0\n",
    "    for column in list(df.columns):\n",
    "        #binarize the columns\n",
    "        if str(df[column][1]).startswith('b'):\n",
    "            unique = df[column].unique()\n",
    "            df[column] = df[column].map({unique[0]: 0, unique[1]: 1})\n",
    "\n",
    "        # eliminate columns with too many NaN values (optional)\n",
    "        # if df[column].isna().sum()/len(df) > 0.85:\n",
    "        #     print(\"% NaN values:\" + str(df[column].isna().sum()/len(df)))\n",
    "        #     df = df.drop([column])\n",
    "    # df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T22:38:48.254768Z",
     "start_time": "2023-10-25T22:38:48.248799Z"
    }
   },
   "outputs": [],
   "source": [
    "def target_type(df):\n",
    "    df = df.dropna()\n",
    "    last = df.columns[-1]\n",
    "    target = df[last]\n",
    "    diff = target.unique()\n",
    "    if len(diff) <= 2:\n",
    "        return \"Binary\"\n",
    "    elif len(diff) > 2:\n",
    "        return \"Multinomial\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-25T22:38:48.272571Z",
     "start_time": "2023-10-25T22:38:48.260395Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_datasets(path):\n",
    "    metadata_df = pd.DataFrame()\n",
    "    num_cols = []\n",
    "    num_rows = []\n",
    "    datasets = {}\n",
    "    names = [] #only the names of the datasets that are included \n",
    "    t_type = []\n",
    "    dataset_names = [f for f in listdir(path) if isfile(join(path, f)) and f.endswith('.arff')]\n",
    "    for i in range(len(dataset_names)):\n",
    "        try:\n",
    "            data = arff.loadarff(path+'/'+dataset_names[i])\n",
    "            df = pd.DataFrame(data[0])\n",
    "            df = preprocess_dataset(df)\n",
    "            name_df = dataset_names[i].replace(\".arff\", \"\")\n",
    "            names.append(name_df)\n",
    "            datasets[name_df] = df\n",
    "            num_cols.append(df.shape[1])\n",
    "            num_rows.append(df.shape[0])\n",
    "            t_type.append(target_type(df))\n",
    "        except:\n",
    "            pass\n",
    "    metadata_df[\"name_dataset\"] = names\n",
    "    metadata_df[\"num_columns\"] = num_cols\n",
    "    metadata_df[\"num_rows\"] = num_rows\n",
    "    metadata_df[\"target_type\"] = t_type\n",
    "    return datasets, metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-25T22:38:49.302325Z",
     "start_time": "2023-10-25T22:38:48.267525Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets, metadata_df = load_datasets(path)\n",
    "# metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T22:38:49.853245Z",
     "start_time": "2023-10-25T22:38:49.314266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'Average Quality': -0.34655532359081415,\n 'Average Coverage': 0.0010438413361169101,\n 'Average Support': 0.0,\n 'WRAcc': -0.00036174877201546366,\n 'Significance': 0.5483384661792094,\n 'Confidence': 0.0,\n 'Number of Subgroups': 50,\n 'Average Length of Subgroups': 0.4}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.nmeef_sd(datasets[\"tic-tac-toe\"], datasets[\"tic-tac-toe\"].columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-25T22:38:54.222634Z",
     "start_time": "2023-10-25T22:38:54.212131Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_algorithms(df, name):\n",
    "    target = df.columns[-1]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"__________________________________________\")\n",
    "    print(\"START Dataset: \", name)\n",
    "    print(\"__________________________________________\")\n",
    "    \n",
    "    # # run sd\n",
    "    # sd = models.sd(df, target)\n",
    "    # print(sd.values())\n",
    "    # print(\"__________________________________________\")\n",
    "\n",
    "    #cn2_sd\n",
    "    print(\"CN2_SD\")\n",
    "    cn2_sd = models.cn2_sd(df, target).values()\n",
    "    print(\"__________________________________________\")\n",
    "    results[\"cn2_sd\"] = list(cn2_sd)\n",
    "\n",
    "    #run sd_map\n",
    "    print(\"SD_MAP\")\n",
    "    sd_map = models.sd_map(df, target, min_support=0.1).values()\n",
    "    print(\"__________________________________________\")\n",
    "    results[\"sd_map\"] = list(sd_map)\n",
    "\n",
    "    #run dssd\n",
    "    print(\"DSSD\")\n",
    "    dssd = models.dssd(df, target, min_support=0.1).values()\n",
    "    print(\"__________________________________________\")\n",
    "    results[\"dssd\"] = list(dssd)\n",
    "\n",
    "    #run nmeef\n",
    "    print(\"NMEEF-SD\")\n",
    "    nmeef = models.nmeef_sd(df, target).values()\n",
    "    print(\"__________________________________________\")\n",
    "    results[\"nnmeef\"] = list(nmeef)\n",
    "\n",
    "    #run apriori\n",
    "    print(\"Apriori-SD\")\n",
    "    a = models.apriori_sd(df, target, min_threshold = 0.1).values()\n",
    "    results[\"a\"] = list(a)\n",
    "    print(\"__________________________________________\")\n",
    "    print(\"END Dataset: \", name)\n",
    "    print(\"__________________________________________\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-25T22:41:55.688669Z",
     "start_time": "2023-10-25T22:38:55.031061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hayes-roth_test\n",
      "__________________________________________\n",
      "START Dataset:  hayes-roth_test\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  hayes-roth_test\n",
      "__________________________________________\n",
      "abalone\n",
      "__________________________________________\n",
      "START Dataset:  abalone\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  abalone\n",
      "__________________________________________\n",
      "haberman\n",
      "__________________________________________\n",
      "START Dataset:  haberman\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  haberman\n",
      "__________________________________________\n",
      "zoo\n",
      "__________________________________________\n",
      "START Dataset:  zoo\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  zoo\n",
      "__________________________________________\n",
      "tic-tac-toe\n",
      "__________________________________________\n",
      "START Dataset:  tic-tac-toe\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  tic-tac-toe\n",
      "__________________________________________\n",
      "iris\n",
      "__________________________________________\n",
      "START Dataset:  iris\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  iris\n",
      "__________________________________________\n",
      "contact-lenses\n",
      "__________________________________________\n",
      "START Dataset:  contact-lenses\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  contact-lenses\n",
      "__________________________________________\n",
      "credit-a\n",
      "__________________________________________\n",
      "START Dataset:  credit-a\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  credit-a\n",
      "__________________________________________\n",
      "car\n",
      "__________________________________________\n",
      "START Dataset:  car\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  car\n",
      "__________________________________________\n",
      "balance-scale\n",
      "__________________________________________\n",
      "START Dataset:  balance-scale\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  balance-scale\n",
      "__________________________________________\n",
      "wisconsin\n",
      "__________________________________________\n",
      "START Dataset:  wisconsin\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  wisconsin\n",
      "__________________________________________\n",
      "cmc\n",
      "__________________________________________\n",
      "START Dataset:  cmc\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  cmc\n",
      "__________________________________________\n",
      "glass\n",
      "__________________________________________\n",
      "START Dataset:  glass\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  glass\n",
      "__________________________________________\n",
      "pima-indians\n",
      "__________________________________________\n",
      "START Dataset:  pima-indians\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  pima-indians\n",
      "__________________________________________\n",
      "hayes-roth_train\n",
      "__________________________________________\n",
      "START Dataset:  hayes-roth_train\n",
      "__________________________________________\n",
      "CN2_SD\n",
      "__________________________________________\n",
      "SD_MAP\n",
      "__________________________________________\n",
      "DSSD\n",
      "__________________________________________\n",
      "NMEEF-SD\n",
      "__________________________________________\n",
      "Apriori-SD\n",
      "__________________________________________\n",
      "END Dataset:  hayes-roth_train\n",
      "__________________________________________\n"
     ]
    }
   ],
   "source": [
    "meta_2 = {}\n",
    "for key in datasets:\n",
    "    if key in [\"dermatology\", \"labor\", \"ionosphere\", \"adult\"]:\n",
    "        continue\n",
    "    print(key)\n",
    "    results = run_algorithms(datasets[key], key)\n",
    "    meta_2[key] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T22:42:13.015333Z",
     "start_time": "2023-10-25T22:42:12.985859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Dataset Algorithm   Quality  Coverage   Support     WRAcc  \\\n",
      "0    hayes-roth_test    cn2_sd -0.958653  0.660714  0.250000 -0.056760   \n",
      "1    hayes-roth_test    sd_map -0.092593  0.250000  0.089286 -0.010204   \n",
      "2    hayes-roth_test      dssd -0.092593  0.250000  0.089286 -0.010204   \n",
      "3    hayes-roth_test    nnmeef -0.236481  0.141429  0.035000 -0.030332   \n",
      "4    hayes-roth_test         a       NaN  0.400000  0.142857       NaN   \n",
      "..               ...       ...       ...       ...       ...       ...   \n",
      "70  hayes-roth_train    cn2_sd -0.652623  0.257576  0.101010  0.001492   \n",
      "71  hayes-roth_train    sd_map  0.000000  0.265152  0.077652  0.017648   \n",
      "72  hayes-roth_train      dssd  0.000000  0.265152  0.077652  0.017648   \n",
      "73  hayes-roth_train    nnmeef -0.157143  0.041667  0.009091 -0.000861   \n",
      "74  hayes-roth_train         a       NaN  0.386364  0.128788       NaN   \n",
      "\n",
      "    Significance  Confidence  # of Subgroups  Length of Rules  \n",
      "0       0.000213    0.296296             2.0             1.00  \n",
      "1            NaN    0.325000             6.0             1.50  \n",
      "2            NaN    0.325000             6.0             1.50  \n",
      "3            NaN    0.245000            50.0             1.04  \n",
      "4            NaN         NaN             3.0             1.00  \n",
      "..           ...         ...             ...              ...  \n",
      "70      0.000040    0.446429             6.0             2.00  \n",
      "71           NaN    0.284091             4.0             1.00  \n",
      "72           NaN    0.284091             4.0             1.00  \n",
      "73           NaN    0.210000            50.0             1.90  \n",
      "74           NaN         NaN             1.0             1.00  \n",
      "\n",
      "[75 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary that contains dictionaries per dataset, and the scores per algorithm.\n",
    "# For each algorithm result, the list represents the scores in these evaluation metrics:\n",
    "# Quality, Coverage, Support, number of subgroups, average length of subgroups (i.e. hte number of rules used to represent a subgroup, on average)\n",
    "meta_2\n",
    "\n",
    "# Convert the nested dictionary into a DataFrame\n",
    "df = pd.DataFrame.from_dict(meta_2, orient='index')\n",
    "df = df.stack().apply(pd.Series).reset_index()\n",
    "\n",
    "# Rename columns for clarity and reordering\n",
    "df.columns = ['Dataset', 'Algorithm', 'Quality', 'Coverage', 'Support', 'WRAcc', \"Significance\", \"Confidence\", '# of Subgroups', 'Length of Rules']\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df[['Dataset', 'Algorithm', 'Quality', 'Coverage', 'Support', 'WRAcc', \"Significance\", \"Confidence\", '# of Subgroups', 'Length of Rules']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T22:42:16.344072Z",
     "start_time": "2023-10-25T22:42:16.333497Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Subgroups Associated with 'High Quality':\n",
      "\n",
      "Rank 1:\n",
      "Subgroup: Binned_Quality=High\n",
      "Support: 32.76%\n",
      "\n",
      "Rank 2:\n",
      "Subgroup: Binned_Quality=High, Binned_WRAcc=High\n",
      "Support: 27.59%\n",
      "\n",
      "Rank 3:\n",
      "Subgroup: Binned_Quality=High, Algorithm=dssd\n",
      "Support: 17.24%\n",
      "\n",
      "Rank 4:\n",
      "Subgroup: Algorithm=sd_map, Binned_Quality=High\n",
      "Support: 15.52%\n",
      "\n",
      "Rank 5:\n",
      "Subgroup: Algorithm=sd_map, Binned_Quality=High, Binned_WRAcc=High\n",
      "Support: 13.79%\n",
      "\n",
      "Rank 6:\n",
      "Subgroup: Binned_Quality=High, Binned_WRAcc=High, Algorithm=dssd\n",
      "Support: 13.79%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- DATA LOADING AND PREPARATION ---\n",
    "\n",
    "# Load the metadata\n",
    "metadata_df = pd.read_csv(\"metadata.csv\")\n",
    "\n",
    "# Convert continuous columns 'Quality' and 'WRAcc' into discrete bins\n",
    "def bin_columns(df):\n",
    "    columns_to_bin = {\n",
    "        'Quality': 'Binned_Quality',\n",
    "        'WRAcc': 'Binned_WRAcc'\n",
    "    }\n",
    "    for col, binned_col in columns_to_bin.items():\n",
    "        bins = [-float('inf'), df[col].quantile(0.33), df[col].quantile(0.67), float('inf')]\n",
    "        labels = ['Low', 'Medium', 'High']\n",
    "        df[binned_col] = pd.cut(df[col], bins=bins, labels=labels, include_lowest=True)\n",
    "    return df\n",
    "\n",
    "metadata_df = bin_columns(metadata_df)\n",
    "\n",
    "# --- TRANSACTIONAL FORMAT CONVERSION ---\n",
    "\n",
    "# Convert dataset to a transactional format suitable for Apriori\n",
    "def convert_to_transactional_format(df):\n",
    "    transactions = []\n",
    "    for _, row in df.iterrows():\n",
    "        transaction = [f\"{col}={value}\" for col, value in row.items() if not pd.isna(value)]\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "selected_columns = ['Dataset', 'Algorithm', 'Binned_Quality', 'Binned_WRAcc']\n",
    "transactional_data = convert_to_transactional_format(metadata_df[selected_columns].dropna())\n",
    "\n",
    "# --- APRIORI FOR FREQUENT ITEMSETS ---\n",
    "\n",
    "# Generate frequent itemsets using the Apriori algorithm\n",
    "def apriori(transactions, min_support=0.1):\n",
    "    # Calculate item frequencies\n",
    "    item_freq = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_freq[item] = item_freq.get(item, 0) + 1\n",
    "\n",
    "    # Filter items by minimum support\n",
    "    items = [item for item, freq in item_freq.items() if freq / len(transactions) >= min_support]\n",
    "\n",
    "    # Generate candidate itemsets\n",
    "    def get_candidates(itemset, length):\n",
    "        return set([i.union(j) for i in itemset for j in itemset if len(i.union(j)) == length])\n",
    "\n",
    "    # Get frequent itemsets\n",
    "    current_set = [frozenset([item]) for item in items]\n",
    "    frequent_itemsets = []\n",
    "    k = 2\n",
    "    while current_set:\n",
    "        valid_sets = []\n",
    "        for itemset in current_set:\n",
    "            count = sum(1 for transaction in transactions if itemset.issubset(transaction))\n",
    "            if count / len(transactions) >= min_support:\n",
    "                valid_sets.append(itemset)\n",
    "                frequent_itemsets.append(itemset)\n",
    "        current_set = get_candidates(valid_sets, k)\n",
    "        k += 1\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "frequent_itemsets = apriori(transactional_data, min_support=0.1)\n",
    "\n",
    "# --- SUBGROUP DISCOVERY ---\n",
    "\n",
    "# Identify and rank subgroups associated with a target value (e.g., 'High Quality')\n",
    "def get_subgroups_for_target(target_item, frequent_itemsets, transactions):\n",
    "    subgroups = [itemset for itemset in frequent_itemsets if target_item in itemset]\n",
    "    subgroup_supports = [\n",
    "        (itemset, sum(1 for transaction in transactions if itemset.issubset(transaction)) / len(transactions))\n",
    "        for itemset in subgroups\n",
    "    ]\n",
    "    ranked_subgroups = sorted(subgroup_supports, key=lambda x: x[1], reverse=True)\n",
    "    return ranked_subgroups\n",
    "\n",
    "target_item = 'Binned_Quality=High'\n",
    "high_quality_subgroups = get_subgroups_for_target(target_item, frequent_itemsets, transactional_data)\n",
    "\n",
    "\n",
    "# --- PRINT RESULTS ---\n",
    "\n",
    "print(\"Ranked Subgroups Associated with 'High Quality':\\n\")\n",
    "for rank, (itemset, support) in enumerate(high_quality_subgroups, start=1):\n",
    "    print(f\"Rank {rank}:\")\n",
    "    print(\"Subgroup:\", ', '.join(itemset))\n",
    "    print(f\"Support: {support:.2%}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T23:26:55.155270Z",
     "start_time": "2023-10-25T23:26:55.124517Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "rtdm",
   "language": "python",
   "display_name": "Python (RTDM)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
